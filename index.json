[{"authors":null,"categories":null,"content":"I am a research scientist in Shanghai / pujiang AI Lab. Currently, I am leading a project team working on multilingual representation learning. Before that, I have been working as a research scientist in ByteDance from 2020 to 2021, working with Lei Li and Hao Zhou. During my PhD (2015-2020), I was luckily supervised by Prof. Xu Sun at Peking University. In the last five years, I have published multiple papers at leading conferences (e.g., ICML, NeurIPS, ACL, EMNLP), and received ACL\u0026#39;21 Best Paper Awards, AAAI\u0026#39;21 New Faculty Highlights Awards. My research interest includes representation learning, multilingual learning, and green NLP.\nGoogle Scholar / Twitter / GitHub\n","date":1640995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1640995200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://jingjingxu.com/author/jingjing-xu-%E8%AE%B8%E6%99%B6%E6%99%B6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jingjing-xu-%E8%AE%B8%E6%99%B6%E6%99%B6/","section":"authors","summary":"I am a research scientist in Shanghai / pujiang AI Lab. Currently, I am leading a project team working on multilingual representation learning. Before that, I have been working as a research scientist in ByteDance from 2020 to 2021, working with Lei Li and Hao Zhou.","tags":null,"title":"Jingjing Xu (ËÆ∏Êô∂Êô∂)","type":"authors"},{"authors":null,"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"483e1e299f148d0e5b3f028f6e42b280","permalink":"https://jingjingxu.com/author/%E8%AE%B8%E6%99%B6%E6%99%B6/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E8%AE%B8%E6%99%B6%E6%99%B6/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"ËÆ∏Êô∂Êô∂","type":"authors"},{"authors":null,"categories":null,"content":" Table of Contents What you will learn Program overview Courses in this program Meet your instructor FAQs What you will learn Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program Python basics Build a foundation in Python. Visualization Learn how to visualize data with Plotly. Statistics Introduction to statistics for data science. Meet your instructor Jingjing Xu (ËÆ∏Êô∂Êô∂) FAQs Are there prerequisites? There are no prerequisites for the first course.\nHow often do the courses run? Continuously, at your own pace.\nBegin the course ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://jingjingxu.com/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n1-2 hours per week, for 8 weeks\nLearn Quiz What is the difference between lists and tuples? Lists\nLists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, \u0026#39;Hello world\u0026#39;] Tuples\nTuples are immutable - they can‚Äôt be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, \u0026#39;Hello world\u0026#39;) Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://jingjingxu.com/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n1-2 hours per week, for 8 weeks\nLearn Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\nWrite Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026#34;country == \u0026#39;Canada\u0026#39;\u0026#34;) fig = px.bar(data_canada, x=\u0026#39;year\u0026#39;, y=\u0026#39;pop\u0026#39;) fig.show() ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://jingjingxu.com/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\nThe parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$. Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://jingjingxu.com/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://jingjingxu.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Zaixiang Zheng","Hao Zhou","Shujian Huang","Jiajun Chen","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Lei Li"],"categories":null,"content":" ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"a5fb0b33f4a46e2cc6ebfdfc4d6f6941","permalink":"https://jingjingxu.com/publication/027-duplex/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/publication/027-duplex/","section":"publication","summary":"Zaixiang Zheng, Hao Zhou, Shujian Huang, Jiajun Chen, **Jingjing Xu**, Lei Li","tags":null,"title":"Duplex Sequence-to-Sequence Learning for Reversible Machine Translation. NeurIPS 2021.","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Hao Zhou","Chun Gan","Zaixiang Zheng","Lei Li"],"categories":null,"content":" ","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"fcc8fa376105fd919d6c031098f2e8fe","permalink":"https://jingjingxu.com/publication/026-volt/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/publication/026-volt/","section":"publication","summary":"**Jingjing Xu**, Hao Zhou, Chun Gan, Zaixiang Zheng, Lei Li","tags":null,"title":"Vocabulary Learning via Optimal Transport for Neural Machine Translation. ACL 2021 (Best Paper).","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Wangchunshu Zhou","Zhiyi Fu","Hao Zhou","Lei Li"],"categories":null,"content":" ","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"f04514716eb8853b9bb3dbfb9ec6d79a","permalink":"https://jingjingxu.com/publication/025-greennlpsurvey/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/publication/025-greennlpsurvey/","section":"publication","summary":"**Jingjing Xu**, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, Lei Li","tags":null,"title":"A Survey on Green Deep Learning. Arxiv 2021. ","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Liang Zhao","Junyang Lin","Rundong Gao","Xu Sun","Hongxia Yang"],"categories":null,"content":" ","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625097600,"objectID":"d224a07ef2b962f6ec1aef6b7986d28e","permalink":"https://jingjingxu.com/publication/024-knas/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/024-knas/","section":"publication","summary":"**Jingjing Xu**, Liang Zhao, Junyang Lin, Rundong Gao, Xu Sun, Hongxia Yang","tags":null,"title":"KNAS: Green Neural Architecture Search. ICML 2021. ","type":"publication"},{"authors":["Wei Li","Ruihan Bao","Keiko Harimoto","Deli Chen","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Qi Su"],"categories":null,"content":" ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"cc6e7d1b2c6ffe40468af38054033d0b","permalink":"https://jingjingxu.com/publication/022-stock/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/022-stock/","section":"publication","summary":"Wei Li, Ruihan Bao, Keiko Harimoto, Deli Chen, **Jingjing Xu**, Qi Su","tags":null,"title":"Modeling the Stock Relation with Graph Network for Overnight Stock Movement Prediction. IJCAI 2020.","type":"publication"},{"authors":["Yiran Chen","Zhenqiao Song","Xianze Wu","Danqing Wang","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Jiaze Chen","Hao Zhou","Lei Li"],"categories":null,"content":" ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"29efe46afe6a68e04a9e65c814ea3bc5","permalink":"https://jingjingxu.com/publication/023-mtg/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/023-mtg/","section":"publication","summary":"Yiran Chen, Zhenqiao Song, Xianze Wu, Danqing Wang, **Jingjing Xu**, Jiaze Chen, Hao Zhou, Lei Li","tags":null,"title":"MTG: A Benchmarking Suite for Multilingual Text Generation. Arxiv 2021. ","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It‚Äôs a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more The template is mobile first with a responsive design to ensure that your site looks stunning on every device. Get Started üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Guide and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy‚Äôs future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you‚Äôll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://jingjingxu.com/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)"],"categories":["News"],"content":"VOLT is a research work exploring how to generate the optimal vocabulary for neural machine translation. In recent days, this paper got a lot of attentions and we also received several questions from readers. To help more readers understand our work better, I write a blog. In this blog, I will try my best to explain the formulations and motivations behind VOLT. I will use easy-to-understand examples for illustration (maybe not very rigorous for paper writing but somehow ok for blogs). If you have more questions, please feel free to contact us. You can find our e-mails on the published paper.\n","date":1597276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597276800,"objectID":"8809df18285b86c4992b99415da447d1","permalink":"https://jingjingxu.com/news/getting-started/","publishdate":"2020-08-13T00:00:00Z","relpermalink":"/news/getting-started/","section":"news","summary":"VOLT is a research work exploring how to generate the optimal vocabulary for neural machine translation. In recent days, this paper got a lot of attentions and we also received several questions from readers.","tags":null,"title":"Our paper got ACL'21 Best Paper Award","type":"news"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)"],"categories":["News"],"content":"This year, AAAI launched a new invited speaker program highlighting AI researchers who have just begun careers as new faculty members or the equivalent in industry. Applications were adjudicated by a committee consisting of the AAAI-21 chairs and a diverse group of AAAI Fellows.\nVery excited to be selected as one of highlighted speakers.\nVideo: https://slideslive.com/38952032\nYou also can find other speakers at:\nYonatan Belinkov, Technion\nVideo: https://slideslive.com/38952033\nPascal Bercher, Australian National University\nVideo: https://slideslive.com/38952027\nNoam Brown, Facebook AI Research\nVideo: https://slideslive.com/38952037\nEunsol Choi, University of Texas, Austin\nVideo: https://slideslive.com/38952036\nSimon S. Du, University of Washington\nVideo: https://slideslive.com/38952024\nChelsea Finn, Stanford University\nVideo: https://slideslive.com/38952022\nJakob Foerster, Facebook AI Research/University of Toronto/Vector Institute\nVideo: https://slideslive.com/38952035\nHoda Heidari, Carnegie Mellon University\nVideo: https://slideslive.com/38952023\nKokil Jaidka, National University of Singapore\nVideo: https://slideslive.com/38952026\nDinesh Jayaraman, University of Pennsylvania\nVideo: https://slideslive.com/38952020\nNadin Kokciyan, University of Edinburgh\nVideo: https://slideslive.com/38952031\nJundong Li, University of Virginia\nVideo: https://slideslive.com/38952029\nHang Ma, Simon Fraser University\nVideo: https://slideslive.com/38952030\nLili Mou, University of Alberta\nVideo: https://slideslive.com/38952028\nJingjing Xu, ByteDance AI Lab\nVideo: https://slideslive.com/38952032\nDiyi Yang, Georgia Institute of Technology\nVideo: https://slideslive.com/38952021\n","date":1589328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589328000,"objectID":"4a43a679b7e38e4d86f9d933ae2dcfba","permalink":"https://jingjingxu.com/news/aaai-faculty/","publishdate":"2020-05-13T00:00:00Z","relpermalink":"/news/aaai-faculty/","section":"news","summary":"This year, AAAI launched a new invited speaker program highlighting AI researchers who have just begun careers as new faculty members or the equivalent in industry. Applications were adjudicated by a committee consisting of the AAAI-21 chairs and a diverse group of AAAI Fellows.","tags":null,"title":"Got AAAI New Faculty Highlights Awards","type":"news"},{"authors":["Liang Zhao","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Junyang Lin","Yichang Zhang","Hongxia Yang","Xu Sun"],"categories":null,"content":" ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"5c6f7fc084e889bd667a4d72c21cad6c","permalink":"https://jingjingxu.com/publication/020-graphgenerate/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/020-graphgenerate/","section":"publication","summary":"Liang Zhao, **Jingjing Xu**, Junyang Lin, Yichang Zhang, Hongxia Yang, Xu Sun","tags":null,"title":"Graph-based Multi-hop Reasoning for Long Text Generation. Arxiv 2020.","type":"publication"},{"authors":["Shangwen Lv","Daya Guo","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Duyu Tang","Nan Duan","Ming Gong","Linjun Shou","Daxin Jiang","Guihong Cao","Songlin Hu"],"categories":null,"content":" ","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"b6d839690c0e5a7b46002e50d2dccf63","permalink":"https://jingjingxu.com/publication/021-graphreasoning/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/publication/021-graphreasoning/","section":"publication","summary":"Commonsense question answering aims to answer questions which require background knowledge that is not explicitly expressed in the question. The key challenge is how to obtain evidence from external knowledge and make predictions based on the evidence. Recent works either learn to generate evidence from human-annotated evidence which is expensive to collect, or extract evidence from either structured or unstructured knowledge bases which fails to take advantages of both sources. In this work, we propose to automatically extract evidence from heterogeneous knowledge sources, and answer questions based on the extracted evidence. Specifically, we extract evidence from both structured knowledge base (i.e. ConceptNet) and Wikipedia plain texts. We construct graphs for both sources to obtain the relational structures of evidence. Based on these graphs, we propose a graph-based approach consisting of a graph-based contextual word representation learning module and a graph-based inference module. The first module utilizes graph structural information to re-define the distance between words for learning better contextual word representations. The second module adopts graph convolutional network to encode neighbor information into the representations of nodes, and aggregates evidence with graph attention mechanism for predicting the final answer. Experimental results on CommonsenseQA dataset illustrate that our graph-based approach over both knowledge sources brings improvement over strong baselines. Our approach achieves the state-of-the-art accuracy (75.3%) on the CommonsenseQA leaderboard.","tags":null,"title":"Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering. AAAI 2020.","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Zhiyuan Zhang","Guangxiang Zhao","Xu Sun"],"categories":null,"content":" ","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"08e6bccd6ef40a2e259db6242da7c8c9","permalink":"https://jingjingxu.com/publication/019-adanorm/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/019-adanorm/","section":"publication","summary":"Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm. Many of previous studies believe that the success of LayerNorm comes from forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets.","tags":null,"title":" Understanding and Improving Layer Normalization. NeurIPS 2019.","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Yuechen Wang","Duyu Tang","Nan Duan","Qi Zeng","Pengcheng Yang","Ming Zhou","Xu Sun"],"categories":null,"content":" ","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"b52635677566758291661f9c56af9ded","permalink":"https://jingjingxu.com/publication/018-clarification/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/publication/018-clarification/","section":"publication","summary":"The ability to ask clarification questions is essential for knowledge-based question answering (KBQA) systems, especially for handling ambiguous phenomena. Despite its importance, clarification has not been well explored in current KBQA systems. Further progress requires supervised resources for training and evaluation, and powerful models for clarification-related text understanding and generation. In this paper, we construct a new clarification dataset, CLAQUA, with nearly 40K open-domain examples. The dataset supports three serial tasks. given a question, identify whether clarification is needed; if yes, generate a clarification question; then predict answers base on external user feedback. We provide representative baselines for these tasks and further introduce a coarse-to-fine model for clarification question generation. Experiments show that the proposed model achieves better performance than strong baselines. The further analysis demonstrates that our dataset brings new challenges and there still remain several unsolved problems, like reasonable automatic evaluation metrics for clarification question generation and powerful models for handling entity sparsity.","tags":null,"title":"Asking Clarification Questions in Knowledge-Based Question Answering. EMNLP 2019.","type":"publication"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you‚Äôll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Charts Academic supports the popular Plotly chart format.\nSave your Plotly JSON in your page folder, for example chart.json, and then add the {{\u0026lt; chart data=\u0026#34;chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}{n}) - \\nabla F(\\mathbf{x}{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\\\\\ math linebreak:\n$$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\\\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ renders as\n$$f(k;p_{0}^{}) = \\begin{cases}p_{0}^{} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ``` renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ``` renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else renders as\nWrite math example Write diagram example Do something else Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell | renders as\nFirst Header Second Header Content Cell Content Cell Content Cell Content Cell Callouts Academic supports a shortcode for ‚Ä¶","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://jingjingxu.com/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Liang Zhao","Hanqi Yan","Qi Zeng","Yun Liang","Xu Sun"],"categories":null,"content":" ","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"bed9a3b12045c5b05628a26834db9abe","permalink":"https://jingjingxu.com/publication/017-lexicalat/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/publication/017-lexicalat/","section":"publication","summary":"Recent work has shown that current text classification models are fragile and sensitive to simple perturbations. In this work, we propose a novel adversarial training approach, LexicalAT, to improve the robustness of current classification models. The proposed approach consists of a generator and a classifier. The generator learns to generate examples to attack the classifier while the classifier learns to defend these attacks. Considering the diversity of attacks, the generator uses a large-scale lexical knowledge base, WordNet, to generate attacking examples by replacing some words in training examples with their synonyms (e.g., sad and unhappy), neighbor words (e.g., fox and wolf), or super-superior words (e.g., chair and armchair). Due to the discrete generation step in the generator, we use policy gradient, a reinforcement learning approach, to train the two modules. Experiments show LexicalAT outperforms strong baselines and reduces test errors on various neural networks, including CNN, RNN, and BERT.","tags":null,"title":"LexicalAT: Lexical-Based Adversarial Reinforcement Training for Robust Sentiment Classification. EMNLP 2019. ","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post‚Äôs folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://jingjingxu.com/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://jingjingxu.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Pengcheng Yang","Junyang Lin","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Jun Xie","Qi Su","Xu Sun"],"categories":null,"content":" ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"20ab014633fb7f5f6933e436a7e472bc","permalink":"https://jingjingxu.com/publication/010-sentimentmodification/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/010-sentimentmodification/","section":"publication","summary":"The task of unsupervised sentiment modification aims to reverse the sentiment polarity of the input text while preserving its semantic content without any parallel data. Most previous work follows a two-step process. They first separate the content from the original sentiment, and then directly generate text with the target sentiment only based on the content produced by the first step. However, the second step bears both the target sentiment addition and content reconstruction, thus resulting in a lack of specific information like proper nouns in the generated text. To remedy this, we propose a specificity-driven cascading approach in this work, which can effectively increase the specificity of the generated text and further improve content preservation. In addition, we propose a more reasonable metric to evaluate sentiment modification. The experiments show that our approach outperforms competitive baselines by a large margin, which achieves 11% and 38% relative improvements of the overall metric on the Yelp and Amazon datasets, respectively.","tags":null,"title":" Specificity-driven Cascading Approach for Unsupervised Sentiment Modification. EMNLP 2019. ","type":"publication"},{"authors":["Shu Liu","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Xuancheng Ren","Xu Sun"],"categories":null,"content":" ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"e84baa89024350a3bbe784e2be889bdd","permalink":"https://jingjingxu.com/publication/011-hownet/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/011-hownet/","section":"publication","summary":"Automatic evaluation of semantic rationality is an important yet challenging task, and current automatic techniques cannot well identify whether a sentence is semantically rational. The methods based on the language model do not measure the sentence by rationality but by commonness. The methods based on the similarity with human written sentences will fail if human-written references are not available. In this paper, we propose a novel model called Sememe-Word-Matching Neural Network (SWM-NN) to tackle semantic rationality evaluation by taking advantage of sememe knowledge base HowNet. The advantage is that our model can utilize a proper combination of sememes to represent the fine-grained semantic meanings of a word within the specific contexts. We use the fine-grained semantic representation to help the model learn the semantic dependency among words. To evaluate the effectiveness of the proposed model, we build a large-scale rationality evaluation dataset. Experimental results on this dataset show that the proposed model outperforms the competitive baselines with a 5.4\\% improvement in accuracy.","tags":null,"title":"Evaluating Semantic Rationality of a Sentence: A SememeWord-Matching Neural Network based on Hownet. NLPCC 2019. ","type":"publication"},{"authors":["Yibo Sun","Duyu Tang","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Nan Duan","Xiaocheng Feng","Bing Qin","Ting Liu","Ming Zhou"],"categories":null,"content":" ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7a8041a82c97327b5fdb527341f068d9","permalink":"https://jingjingxu.com/publication/012-knowledgeqa/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/012-knowledgeqa/","section":"publication","summary":"Conversational semantic parsing over tables requires knowledge acquiring and reasoning abilities, which have not been well explored by current state-of-the-art approaches. Motivated by this fact, we propose a knowledgeaware semantic parser to improve parsing performance by integrating various types of knowledge. In this paper, we consider three types of knowledge, including grammar knowledge, expert knowledge, and external resource knowledge. First, grammar knowledge empowers the model to effectively replicate previously generated logical form, which effectively handles the co-reference and ellipsis phenomena in conversation Second, based on expert knowledge, we propose a decomposable model, which is more controllable compared with traditional end-toend models that put all the burdens of learning on trial-and-error in an end-to-end way. Third, external resource knowledge, i.e., provided by a pre-trained language model or an entity typing model, is used to improve the representation of question and table for a better semantic understanding. We conduct experiments on the SequentialQA dataset. Results show that our knowledge-aware model outperforms the state-of-the-art approaches. Incremental experimental results also prove the usefulness of various knowledge. Further analysis shows that our approach has the ability to derive the meaning representation of a context-dependent utterance by leveraging previously generated outcomes.","tags":null,"title":"Knowledge-Aware Conversational Semantic Parsing over Web Tables. NLPCC 2019. ","type":"publication"},{"authors":["Pengcheng Yang","Fuli Luo","Shuangzhi Wu","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Dongdong Zhang"],"categories":null,"content":" ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"268e66df8c39115e65e5eb480a032665","permalink":"https://jingjingxu.com/publication/013-semanticmapping/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/013-semanticmapping/","section":"publication","summary":"Cross-lingual word embeddings aim to capture common linguistic regularities of different languages, which benefit various downstream tasks ranging from machine translation to transfer learning. Recently, it has been shown that these embeddings can be effectively learned by aligning two disjoint monolingual vector spaces through a linear transformation (word mapping). In this work, we focus on learning such a word mapping without any supervision signal. Most previous work of this task adopts parametric metrics to measure distribution differences, which typically requires a sophisticated alternate optimization process, either in the form of \\emph{minmax game} or intermediate \\emph{density estimation}. This alternate optimization process is relatively hard and unstable. In order to avoid such sophisticated alternate optimization, we propose to learn unsupervised word mapping by directly maximizing the mean discrepancy between the distribution of transferred embedding and target embedding. Extensive experimental results show that our proposed model outperforms competitive baselines by a large margin.","tags":null,"title":"Learning Unsupervised Word Mapping via Maximum Mean Discrepancy. NLPCC 2019. ","type":"publication"},{"authors":["Guangxiang Zhao","Xu Sun","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Zhiyuan Zhang","Liangchen Luo"],"categories":null,"content":" ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"37aa044a1f2281bad5eb63a902a7f6ee","permalink":"https://jingjingxu.com/publication/014-muse/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/014-muse/","section":"publication","summary":"Guangxiang Zhao, Xu Sun, **Jingjing Xu**, Zhiyuan Zhang, Liangchen Luo","tags":null,"title":"Muse: Parallel Multi-scale Attention for Sequence to Sequence Learning. Arxiv 2019. ","type":"publication"},{"authors":["Wanjun Zhong","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Duyu Tang","Zenan Xu","Nan Duan","Ming Zhou","Jiahai Wang","Jian Yin"],"categories":null,"content":" ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"127235af0d69173434f522e586c24a70","permalink":"https://jingjingxu.com/publication/015-fever/","publishdate":"2019-02-01T00:00:00Z","relpermalink":"/publication/015-fever/","section":"publication","summary":"Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence. In this work, we present a method suitable for reasoning about the semantic-level structure of evidence. Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling. We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet. Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances. Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph. We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy. Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.","tags":null,"title":"Reasoning over Semantic-level Graph for Fact Checking. ACL 2019.","type":"publication"},{"authors":["Guangxiang Zhao","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Qi Zeng","Xuancheng Ren"],"categories":null,"content":" ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"77ab20e432075dcaf01f8919a3f3bbbc","permalink":"https://jingjingxu.com/publication/016-musicreview/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/016-musicreview/","section":"publication","summary":"This paper explores a new natural languageprocessing task, review-driven multi-label musicstyle classification. This task requires systemsto identify multiple styles of music basedon its reviews on websites. The biggest challengelies in the complicated relations of musicstyles. To tackle this problem, we proposea novel deep learning approach to automaticallylearn and exploit style correlations.Experiment results show that our approachachieves large improvements over baselines onthe proposed dataset. Furthermore, the visualizedanalysis shows that our approach performswell in capturing style correlations.","tags":null,"title":"Review-Driven Multi-Label Music Style Classification by Exploiting Style Correlations. NAACL 2019. ","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Xu Sun","Qi Zeng","Xuanchen Ren","Xiaodong Zhang","Houfeng Wang","Weijie Li"],"categories":null,"content":" ","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"e0c32818126b97acb138200af8747612","permalink":"https://jingjingxu.com/publication/009-styletransfer/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/009-styletransfer/","section":"publication","summary":"The goal of sentiment-to-sentiment ‚Äútranslation‚Äù is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of parallel data. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art systems. Especially, the proposed method substantially improves the content preservation performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.","tags":null,"title":"Unpaired Sentiment-to-Sentiment Translation Using Cycled Reinforcement Learning. ACL 2018.","type":"publication"},{"authors":null,"categories":null,"content":"Add your content here‚Ä¶\n","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530144000,"objectID":"665288c8761d48eb3366c37954243edc","permalink":"https://jingjingxu.com/gallery/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/gallery/","section":"","summary":"Here we describe how to add a page to your site.","tags":null,"title":"An example title","type":"page"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Yi Zhang","Qi Zeng","Xuancheng Ren","Xiaoyan Cai","Xu Sun"],"categories":null,"content":" ","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"3605aff7ad144b5052ccd7a1d2c3c970","permalink":"https://jingjingxu.com/publication/007-skeleton/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/007-skeleton/","section":"publication","summary":"Narrative story generation is a challenging problem because it demands the generated sentences with tight semantic connections, which has not been well studied by most existing generative models. To address this problem, we propose a skeleton-based model to promote the coherence of generated stories. Different from traditional models that generate a complete sentence at a stroke, the proposed model first generates the most critical phrases, called skeleton, and then expands the skeleton to a complete and fluent sentence. The skeleton is not manually defined, but learned by a reinforcement learning method. Compared to the state-of-the-art models, our skeleton-based model can generate significantly more coherent text according to human evaluation and automatic evaluation. The G-score is improved by 20.1% in human evaluation.","tags":null,"title":"A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation. EMNLP 2018. ","type":"publication"},{"authors":["Liangchen Luo","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Junyang Lin","Qi Zeng","Xu Sun"],"categories":null,"content":" ","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"0e45b4c7bfafbad1af7ea4b0bc8978f2","permalink":"https://jingjingxu.com/publication/008-automatching/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/008-automatching/","section":"publication","summary":"Generating semantically coherent responses is still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a relation between the whole meanings of inputs and outputs. To address this problem, we propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model contains two auto-encoders and one mapping module. The auto-encoders learn the semantic representations of inputs and responses, and the mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models.","tags":null,"title":"An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation. EMNLP 2018. ","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Xuancheng Ren","Junyang Lin","Xu Sun"],"categories":null,"content":" ","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"848d73c80c9022f99f788f0351b6adcc","permalink":"https://jingjingxu.com/publication/009-dpgan/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/009-dpgan/","section":"publication","summary":"Existing text generation methods tend to produce repeated and \"boring\" expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for \"novel\" and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel language-model based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.","tags":null,"title":"DP-GAN: Diversity-Promoting Generative Adversarial Network for Generating Informative and Diversified Text. EMNLP 2018. ","type":"publication"},{"authors":["Yi Zhang","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Pengcheng Yang","Xu Sun"],"categories":null,"content":" ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"13e939ead3ca4891178df0bbb7d5ab5b","permalink":"https://jingjingxu.com/publication/004-sentimentmemory/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/004-sentimentmemory/","section":"publication","summary":"The task of sentiment modification requires reversing the sentiment of the input and preserving the sentiment-independent content. However, aligned sentences with the same content but different sentiments are usually unavailable. Due to the lack of such parallel data, it is hard to extract sentiment independent content and reverse the sentiment in an unsupervised way. Previous work usually can not reconcile sentiment transformation and content preservation. In this paper, motivated by the fact the non-emotional context (e.g., ‚Äústaff‚Äù) provides strong cues for the occurrence of emotional words (e.g., ‚Äúfriendly‚Äù), we propose a novel method that automatically extracts appropriate sentiment information from learned sentiment memories according to the specific context. Experiments show that our method substantially improves the content preservation degree and achieves the state-of-the-art performance.","tags":null,"title":" Learning Sentiment Memories for Sentiment Modification without Parallel Data. EMNLP 2018. ","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Hangfeng He","Xu Sun","Xuancheng Ren","Sujian Li"],"categories":null,"content":" ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"ed11b67aa137700d5902c70ce051a72f","permalink":"https://jingjingxu.com/publication/005-ner/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/005-ner/","section":"publication","summary":"Named entity recognition (NER) in Chinese social media is an important, but challenging task because Chinese social media language is informal and noisy. Most previous methods on NER focus on in-domain supervised learning, which is limited by scarce annotated data in social media. In this paper, we present that sufficient corpora in formal domains and massive unannotated text can be combined to improve the NER performance in social media. We propose a unified model which can learn from out-of-domain corpora and in-domain unannotated text. The unified model is composed of two parts. One is for cross-domain learning and the other is for semisupervised learning. Cross-domain learning can learn out-of-domain information based on domain similarity. Semisupervised learning can learn in-domain unannotated information by self-training. Experimental results show that our unified model yields a 9.57% improvement over strong baselines and achieves the state-of-the-art performance.","tags":null,"title":"Cross-Domain and Semi-supervised Named Entity Recognition in Chinese Social Media: A Unified Model. TASLP 2018. ","type":"publication"},{"authors":["Xu Sun","Xuancheng Ren","Shuming Ma","Bingzhen Wei","Wei Li","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Houfeng Wang","Yi Zhang"],"categories":null,"content":" ","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d1ebb481eb773c84cf0d780991b8eec9","permalink":"https://jingjingxu.com/publication/006-tkde/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/006-tkde/","section":"publication","summary":"We propose a simple yet effective technique to simplify the training and the resulting model of neural networks. In back propagation, only a small subset of the full gradient is computed to update the model parameters. The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept. As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction in the computational cost. Based on the sparsified gradients, we further simplify the model by eliminating the rows or columns that are seldom updated, which will reduce the computational cost both in the training and decoding, and potentially accelerate decoding in real-world applications. Surprisingly, experimental results demonstrate that most of time we only need to update fewer than 5% of the weights at each back propagation pass. More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given. The model simplification results show that we could adaptively simplify the model which could often be reduced by around 9x, without any loss on accuracy or even with improved accuracy.","tags":null,"title":"Training Simplification and Model Simplification for Deep Learning: A Minimal Effort Back Propagation Method. TKDE 2018. ","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Xu Sun"],"categories":null,"content":" ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"27b04db70233bfba3e475ddc675362e1","permalink":"https://jingjingxu.com/publication/001-segmentationacl2016/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/001-segmentationacl2016/","section":"publication","summary":"Recently, many neural network models have been applied to Chinese word segmentation. However, such models focus more on collecting local information while long distance dependencies are not well learned. To integrate local features with long distance dependencies, we propose a dependency-based gated recursive neural network. Local features are first collected by bi-directional long short term memory network, then combined and refined to long distance dependencies via gated recursive neural network. Experimental results show that our model is a competitive model for Chinese word segmentation.","tags":null,"title":"Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation. ACL 2016.","type":"publication"},{"authors":["Shuming Ma","Jingjing Xu (ËÆ∏Êô∂Êô∂)","Yi Zhang","Houfeng Wang","Wei Li","Qi Su","Xu Sun"],"categories":null,"content":" ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"493fc8566534c973ef89ac9a1f6828d5","permalink":"https://jingjingxu.com/publication/002-summarization/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/002-summarization/","section":"publication","summary":"Current Chinese social media text summarization models are based on an encoder-decoder framework. Although its generated summaries are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and summaries for Chinese social media summarization. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms baseline systems on a social media corpus.","tags":null,"title":"Improving Semantic Relevance for Sequence-to-Sequence Learning of Chinese Social Media Text Summarization. ACL 2017.","type":"publication"},{"authors":["Jingjing Xu (ËÆ∏Êô∂Êô∂)","Shuming Ma","Yi Zhang","Bingzhen Wei","Xiaoyan Cai","Xu Sun"],"categories":null,"content":" ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"ef510d4c2de6920f6a32d4364b2688f7","permalink":"https://jingjingxu.com/publication/003-segmentationnlpcc/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/003-segmentationnlpcc/","section":"publication","summary":"Recent studies have shown effectiveness in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. We propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data. Experiment results show that our work significantly improves the performance on low-resource datasets, 2.3% and 1.5% Fscore on PKU and CTB datasets. Furthermore, this paper achieves stateof-the-art results, 96.1%, and 96.2% F-score on PKU and CTB datasets.","tags":null,"title":"Transfer Deep Learning for Low-Resource Chinese Word Segmentation with A Novel Neural Network. NLPCC 2017.","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://jingjingxu.com/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://jingjingxu.com/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://jingjingxu.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]